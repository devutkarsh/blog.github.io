<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=theme-color content="dark"><title>FinOps 101 - Saving Databricks Cost | Dev Utkarsh</title><script async src="https://www.googletagmanager.com/gtag/js?id=G-NMJNTL3ZZN"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-NMJNTL3ZZN",{anonymize_ip:!1})}</script><script type=application/javascript>var doNotTrack=!1;doNotTrack||(window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga("create","G-NMJNTL3ZZN","auto"),ga("send","pageview"))</script><script async src=https://www.google-analytics.com/analytics.js></script><meta property="og:url" content="https://blog.devutkarsh.com/finops-101-saving-databricks-cost/"><meta property="og:site_name" content="Dev Utkarsh Blog"><meta property="og:title" content="FinOps 101 - Saving Databricks Cost | Dev Utkarsh"><meta property="og:description" content=" FinOps for Data Engineers: Stop Wasting Money by Using Databricks Smarter "><meta itemprop=name content="FinOps 101 - Saving Databricks Cost | Dev Utkarsh"><meta name=twitter:title content="FinOps 101 - Saving Databricks Cost | Dev Utkarsh"><meta name=application-name content="FinOps 101 - Saving Databricks Cost | Dev Utkarsh"><meta name=description content="FinOps for Data Engineers: Stop Wasting Money by Using Databricks Smarter"><meta name=twitter:description content="FinOps for Data Engineers: Stop Wasting Money by Using Databricks Smarter "><meta itemprop=description content=" FinOps for Data Engineers: Stop Wasting Money by Using Databricks Smarter "><meta property="og:image" content="https://blog.devutkarsh.com/assets/images/tech/finops101-databricks.png"><meta property="og:type" content="article"><meta property="article:publisher" content="Dev Utkarsh"><meta property="og:article:published_time" content="2025-05-06T00:00:00Z"><meta property="article:published_time" content="2025-05-06T00:00:00Z"><script defer type=application/ld+json>{"@context":"http://schema.org","@type":"Article","headline":"FinOps 101 - Saving Databricks Cost","author":{"@type":"Person","name":"Dev Utkarsh"},"datePublished":"2025-05-06","description":"FinOps for Data Engineers: Stop Wasting Money by Using Databricks Smarter","wordCount":627,"mainEntityOfPage":"True","dateModified":"2025-05-06","publisher":{"@type":"Organization","name":"Dev Utkarsh","logo":{"@type":"imageObject","url":"https:\/\/blog.devutkarsh.com\/favicon.ico"}}}</script><link rel="shortcut icon" type=image/x-icon href=/favicon.ico><link rel=stylesheet href=/sass/main.min.f13493bc932ede4e3db6190b3b3f1f70d3585e010477a450a433a4d4d7dab03f.css><style>:root{--article-font-family:"Titillium Web", var(--base-font-family)}</style><script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Titillium+Web:wght@400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script><div id=fb-root></div></head><script>(function(){const t="ThemeColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="dark"&&n?document.documentElement.dataset.userColorScheme="dark":document.documentElement.dataset.userColorScheme="light"})()</script><body class=dark><nav class=navbar><div class=container><div class=flex><div><a class=brand href=/>Dev Utkarsh</a></div><div class=flex><a href=/articles/>Articles</a>
<a href=https://devutkarsh.com>Music</a>
<button id=dark-mode-button><svg class="light" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" width="1em" height="1em" style="-ms-transform:rotate(360deg);-webkit-transform:rotate(360deg);transform:rotate(360deg)" viewBox="0 0 36 36"><path fill="#ffd983" d="M30.312.776C32 19 20 32 .776 30.312c8.199 7.717 21.091 7.588 29.107-.429C37.9 21.867 38.03 8.975 30.312.776z"/><path d="M30.705 15.915a1.163 1.163.0 101.643 1.641 1.163 1.163.0 00-1.643-1.641zm-16.022 14.38a1.74 1.74.0 000 2.465 1.742 1.742.0 100-2.465zm13.968-2.147a2.904 2.904.0 01-4.108.0 2.902 2.902.0 010-4.107 2.902 2.902.0 014.108.0 2.902 2.902.0 010 4.107z" fill="#ffcc4d"/><rect x="0" y="0" width="36" height="36" fill="rgba(0, 0, 0, 0)"/></svg><svg class="dark" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false" width="1em" height="1em" style="-ms-transform:rotate(360deg);-webkit-transform:rotate(360deg);transform:rotate(360deg)" viewBox="0 0 36 36"><path fill="#ffd983" d="M16 2s0-2 2-2 2 2 2 2v2s0 2-2 2-2-2-2-2V2zm18 14s2 0 2 2-2 2-2 2h-2s-2 0-2-2 2-2 2-2h2zM4 16s2 0 2 2-2 2-2 2H2s-2 0-2-2 2-2 2-2h2zm5.121-8.707s1.414 1.414.0 2.828-2.828.0-2.828.0L4.878 8.708s-1.414-1.414.0-2.829c1.415-1.414 2.829.0 2.829.0l1.414 1.414zm21 21s1.414 1.414.0 2.828-2.828.0-2.828.0l-1.414-1.414s-1.414-1.414.0-2.828 2.828.0 2.828.0l1.414 1.414zm-.413-18.172s-1.414 1.414-2.828.0.0-2.828.0-2.828l1.414-1.414s1.414-1.414 2.828.0.0 2.828.0 2.828l-1.414 1.414zm-21 21s-1.414 1.414-2.828.0.0-2.828.0-2.828l1.414-1.414s1.414-1.414 2.828.0.0 2.828.0 2.828l-1.414 1.414zM16 32s0-2 2-2 2 2 2 2v2s0 2-2 2-2-2-2-2v-2z"/><circle fill="#ffd983" cx="18" cy="18" r="10"/><rect x="0" y="0" width="36" height="36" fill="rgba(0, 0, 0, 0)"/></svg></button></div></div></div></nav><main><div class=container><article><header class=article-header><div class=thumb><div><h1>FinOps 101 - Saving Databricks Cost</h1><div class=post-meta><div>By Dev Utkarsh | <time>May 06, 2025</time>
| 3 minutes</div><div class=tags><a href=/tags/databricks/>databricks</a>
<a href=/tags/finops/>finops</a>
<a href=/tags/cloud/>cloud</a></div></div></div></div></header></article><div class=article-post><h2 id=finops-for-data-engineers-stop-wasting-money-by-using-databricks-smarter><a href=#finops-for-data-engineers-stop-wasting-money-by-using-databricks-smarter class=anchor><svg class="icon" aria-hidden="true" focusable="false" height="16" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5.0-3-1.69-3-3.5S2.55 3 4 3h4c1.45.0 3 1.69 3 3.5.0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98.0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98.0-2-1.22-2-2.5.0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45.0 3-1.69 3-3.5S14.5 6 13 6z"/></svg></a>FinOps for Data Engineers: Stop Wasting Money by Using Databricks Smarter</h2><p><img loading=lazy src=/assets/images/tech/finops101-databricks.png alt=Kubernetes width=1200 height=800></p><p>Databricks has revolutionized how data teams handle big data, machine learning, and analytics by offering a unified cloud platform with powerful distributed computing. But here’s the tough truth:</p><p>Many companies are burning cloud money unnecessarily because developers are using Databricks notebooks as an all-in-one development environment — even for tasks that don’t need cluster power.</p><p>This article explores why this behavior is costly, how to shift towards local-first development, and how to use the Databricks CLI to push only the final, distributed workloads to the cloud — saving your company potentially thousands of dollars per month.</p><h3 id=the-problem-notebooks-as-ide--unnecessary-cloud-costs><a href=#the-problem-notebooks-as-ide--unnecessary-cloud-costs class=anchor><svg class="icon" aria-hidden="true" focusable="false" height="16" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5.0-3-1.69-3-3.5S2.55 3 4 3h4c1.45.0 3 1.69 3 3.5.0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98.0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98.0-2-1.22-2-2.5.0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45.0 3-1.69 3-3.5S14.5 6 13 6z"/></svg></a>The Problem: Notebooks as IDE = Unnecessary Cloud Costs</h3><p>Notebooks are popular because they’re interactive, visual, and easy to use. But here’s what’s happening across many teams:</p><ul><li><strong>A developer wants to write a small Spark script.</strong></li><li><strong>They open a Databricks notebook.</strong></li><li><strong>The cluster spins up.</strong></li><li><strong>They write some test code, fix syntax errors, retry, debug, iterate.</strong></li><li><strong>They run small test datasets (which could easily fit on a laptop).</strong></li><li><strong>They hit an error or need to fix logic, rerun, reconsume cluster time.</strong></li></ul><blockquote><p>Every minute the cluster runs, the company is paying for cloud compute, often for work that doesn’t need distributed resources. Multiply that by dozens or hundreds of engineers, and you’re staring at runaway Databricks bills.</p></blockquote><h3 id=the-finops-shift-local-first-development--cloud-only-final-runs><a href=#the-finops-shift-local-first-development--cloud-only-final-runs class=anchor><svg class="icon" aria-hidden="true" focusable="false" height="16" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5.0-3-1.69-3-3.5S2.55 3 4 3h4c1.45.0 3 1.69 3 3.5.0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98.0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98.0-2-1.22-2-2.5.0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45.0 3-1.69 3-3.5S14.5 6 13 6z"/></svg></a>The FinOps Shift: Local-First Development + Cloud-Only Final Runs</h3><p>Here’s the FinOps-aware workflow we recommend:</p><p>✅ <strong>Step 1: Develop locally</strong></p><p>Write, debug, and unit-test your Spark/PySpark/Scala code on your laptop. Use tools like:</p><ul><li>Local Jupyter Notebooks or JupyterLab</li><li>VS Code or PyCharm with Spark dependencies</li><li>Small local sample datasets</li></ul><p>This costs you zero cloud dollars.</p><p>✅ <strong>Step 2: Use Databricks CLI for Final Cloud Runs</strong></p><p>Once your code works locally, push it to Databricks clusters only when you need distributed power — i.e., when running on full datasets, leveraging Spark parallelism, or integrating with cloud-specific tools.</p><h3 id=databricks-cli-setup-and-usage><a href=#databricks-cli-setup-and-usage class=anchor><svg class="icon" aria-hidden="true" focusable="false" height="16" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5.0-3-1.69-3-3.5S2.55 3 4 3h4c1.45.0 3 1.69 3 3.5.0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98.0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98.0-2-1.22-2-2.5.0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45.0 3-1.69 3-3.5S14.5 6 13 6z"/></svg></a>Databricks CLI Setup and Usage</h3><p>Here’s how to set up the Databricks CLI and integrate it into your local-to-cloud workflow.</p><p>1️⃣ <strong>Install the Databricks CLI</strong></p><p>First, install the CLI:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-zsh data-lang=zsh><span class=line><span class=cl>pip install databricks-cli
</span></span></code></pre></td></tr></table></div></div><p>2️⃣ <strong>Configure the CLI</strong></p><p>You need a Databricks personal access token. Generate it in your Databricks workspace (Account Settings → User Settings → Access Tokens).</p><p>Then configure the CLI:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-zsh data-lang=zsh><span class=line><span class=cl>databricks configure --token
</span></span></code></pre></td></tr></table></div></div><p>You’ll be prompted for:
• Databricks host (e.g., <a href=https://.databricks.com>https://.databricks.com</a>)
• Access token</p><p>3️⃣ <strong>Use the CLI to Manage Jobs and Notebooks</strong></p><ul><li>Upload local notebook or script</li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-zsh data-lang=zsh><span class=line><span class=cl>databricks workspace import ./my_notebook.py /Users/yourname/my_notebook.py
</span></span></code></pre></td></tr></table></div></div><ul><li>Run a job on Databricks
You can define a job JSON file (job-config.json) like:</li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-json data-lang=json><span class=line><span class=cl><span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;name&#34;</span><span class=p>:</span> <span class=s2>&#34;my-test-job&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;new_cluster&#34;</span><span class=p>:</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=nt>&#34;spark_version&#34;</span><span class=p>:</span> <span class=s2>&#34;13.3.x-scala2.12&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=nt>&#34;node_type_id&#34;</span><span class=p>:</span> <span class=s2>&#34;i3.xlarge&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=nt>&#34;num_workers&#34;</span><span class=p>:</span> <span class=mi>2</span>
</span></span><span class=line><span class=cl>  <span class=p>},</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;notebook_task&#34;</span><span class=p>:</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=nt>&#34;notebook_path&#34;</span><span class=p>:</span> <span class=s2>&#34;/Users/yourname/my_notebook.py&#34;</span>
</span></span><span class=line><span class=cl>  <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></td></tr></table></div></div><ul><li>Then trigger the job:</li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-zsh data-lang=zsh><span class=line><span class=cl>databricks <span class=nb>jobs</span> create --json-file job-config.json
</span></span></code></pre></td></tr></table></div></div><ul><li>Monitor job runs</li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-zsh data-lang=zsh><span class=line><span class=cl>databricks runs list
</span></span><span class=line><span class=cl>databricks runs get --run-id &lt;run-id&gt;
</span></span></code></pre></td></tr></table></div></div><ul><li>Export results or logs</li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-zsh data-lang=zsh><span class=line><span class=cl>databricks fs cp dbfs:/path/to/results ./local_results --recursive
</span></span></code></pre></td></tr></table></div></div><h3 id=benefits-of-local--cli-workflow><a href=#benefits-of-local--cli-workflow class=anchor><svg class="icon" aria-hidden="true" focusable="false" height="16" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5.0-3-1.69-3-3.5S2.55 3 4 3h4c1.45.0 3 1.69 3 3.5.0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98.0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98.0-2-1.22-2-2.5.0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45.0 3-1.69 3-3.5S14.5 6 13 6z"/></svg></a>Benefits of Local + CLI Workflow</h3><p>✅ Cost Savings
You avoid paying cloud cluster costs for development/debugging iterations.</p><p>✅ Faster Feedback
Local testing gives you immediate feedback without cluster spin-up delays.</p><p>✅ Cleaner Code
You push only working, tested code to the cluster, reducing failed runs.</p><p>✅ FinOps Culture
Your team builds a culture of cloud cost awareness — a key competitive advantage as cloud spend grows.</p><h3 id=final-finops-advice---the-conclusion><a href=#final-finops-advice---the-conclusion class=anchor><svg class="icon" aria-hidden="true" focusable="false" height="16" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5.0-3-1.69-3-3.5S2.55 3 4 3h4c1.45.0 3 1.69 3 3.5.0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98.0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98.0-2-1.22-2-2.5.0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45.0 3-1.69 3-3.5S14.5 6 13 6z"/></svg></a>Final FinOps Advice - The Conclusion</h3><p>Databricks is powerful — but with great power comes great responsibility (and costs).</p><p>By adopting a local + CLI workflow, your team can cut waste on iterations, accelerate development, and build a FinOps-aware engineering culture that respects cloud budgets.</p><p>Start today: install the Databricks CLI, shift development locally, and watch your cloud bills drop — without sacrificing innovation or speed.</p><style>#share-buttons{display:inline-block;vertical-align:middle}#share-buttons:after{content:"";display:block;clear:both}#share-buttons>div{position:relative;text-align:left;height:36px;width:32px;float:left;text-align:center}#share-buttons>div>svg{height:16px;fill:#d5d5d5;margin-top:10px}#share-buttons>div:hover{cursor:pointer}#share-buttons>div.facebook:hover>svg{fill:#3b5998}#share-buttons>div.twitter:hover>svg{fill:#55acee}#share-buttons>div.linkedin:hover>svg{fill:#0077b5}#share-buttons>div.pinterest:hover>svg{fill:#cb2027}#share-buttons>div.mail:hover>svg{fill:#7d7d7d}#share-buttons>div.facebook>svg{height:18px;margin-top:9px}#share-buttons>div.twitter>svg{height:20px;margin-top:8px}#share-buttons>div.linkedin>svg{height:19px;margin-top:7px}#share-buttons>div.pinterest>svg{height:20px;margin-top:9px}#share-buttons>div.mail>svg{height:14px;margin-top:11px}</style><span style=color:silver>Share with others on</span><div id=share-buttons><div class=facebook title="Share this on Facebook" onclick='window.open("http://www.facebook.com/share.php?u=https://blog.devutkarsh.com/finops-101-saving-databricks-cost/")'><svg viewBox="0 0 1792 1792" xmlns="http://www.w3.org/2000/svg"><path d="M1343 12v264h-157q-86 0-116 36t-30 108v189h293l-39 296h-254v759H734V905H479V609h255V391q0-186 104-288.5T1115 0q147 0 228 12z"/></svg></div><div class=twitter title="Share this on Twitter" onclick='window.open("http://twitter.com/intent/tweet?text=FinOps 101 - Saving Databricks Cost&url=https://blog.devutkarsh.com/finops-101-saving-databricks-cost/")'><svg viewBox="0 0 1792 1792" xmlns="http://www.w3.org/2000/svg"><path d="M1684 408q-67 98-162 167 1 14 1 42 0 130-38 259.5T1369.5 1125 1185 1335.5t-258 146-323 54.5q-271 0-496-145 35 4 78 4 225 0 401-138-105-2-188-64.5T285 1033q33 5 61 5 43 0 85-11-112-23-185.5-111.5T172 710v-4q68 38 146 41-66-44-105-115t-39-154q0-88 44-163 121 149 294.5 238.5T884 653q-8-38-8-74 0-134 94.5-228.5T1199 256q140 0 236 102 109-21 205-78-37 115-142 178 93-10 186-50z"/></svg></div><div class=linkedin title="Share this on Linkedin" onclick='window.open("https://www.linkedin.com/shareArticle?mini=true&url=https://blog.devutkarsh.com/finops-101-saving-databricks-cost/&title=&summary=&source=")'><svg viewBox="0 0 1792 1792" xmlns="http://www.w3.org/2000/svg"><path d="M477 625v991H147V625h330zm21-306q1 73-50.5 122T312 490h-2q-82 0-132-49t-50-122q0-74 51.5-122.5T314 148t133 48.5T498 319zm1166 729v568h-329v-530q0-105-40.5-164.5T1168 862q-63 0-105.5 34.5T999 982q-11 30-11 81v553H659q2-399 2-647t-1-296l-1-48h329v144h-2q20-32 41-56t56.5-52 87-43.5T1285 602q171 0 275 113.5t104 332.5z"/></svg></div><div class=mail title="Share this through Email" onclick='window.open("mailto:?&body=https://blog.devutkarsh.com/finops-101-saving-databricks-cost/")'><svg viewBox="0 0 1792 1792" xmlns="http://www.w3.org/2000/svg"><path d="M1792 710v794q0 66-47 113t-113 47H160q-66 0-113-47T0 1504V710q44 49 101 87 362 246 497 345 57 42 92.5 65.5t94.5 48 110 24.5h2q51 0 110-24.5t94.5-48 92.5-65.5q170-123 498-345 57-39 1e2-87zm0-294q0 79-49 151t-122 123q-376 261-468 325-10 7-42.5 30.5t-54 38-52 32.5-57.5 27-50 9h-2q-23 0-50-9t-57.5-27-52-32.5-54-38T639 1015q-91-64-262-182.5T172 690q-62-42-117-115.5T0 438q0-78 41.5-130T160 256h1472q65 0 112.5 47t47.5 113z"/></svg></div></div></div></div><div class=container><nav class="flex container suggested"><a rel=prev href=/the-story-of-multi-cloud/ title="Previous post (older)"><span>Previous</span>
The story of Multi-Cloud!</a></nav></div><div class=container></div></main></main><footer class="footer flex"><section class=container><nav class=footer-links><a href=/>© 2021</a></nav></section><script defer src=/ts/features.a585ac4e7c01cd663b5ab5e21c48129b550c46343e9927f0f1f29eeb7a57f6b9.js data-enable-footnotes=true></script></footer></body></html>